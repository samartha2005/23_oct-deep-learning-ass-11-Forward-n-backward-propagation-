{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. Explain the concept of forward propagation in a neural network.\n",
        "\n",
        "Ans:\n",
        "\n",
        "Forward propagation is the process by which input data is passed through a neural network to generate an output. In a neural network, each layer of neurons processes input data, applies transformations, and passes the output to the next layer. Here’s a step-by-step breakdown of forward propagation:\n",
        "\n",
        "Input Layer: The input data is fed into the network through the input layer, which holds the initial values for each feature of the data.\n",
        "\n",
        "Hidden Layers:\n",
        "\n",
        "Each neuron in a hidden layer receives input from neurons in the previous layer.\n",
        "These inputs are weighted (each connection has an associated weight).\n",
        "The weighted inputs are summed and then passed through an activation function (such as ReLU, sigmoid, or tanh) to introduce non-linearity.\n",
        "The output of each neuron is passed to the next layer.\n",
        "Output Layer:\n",
        "\n",
        "In the final layer, the neurons' outputs represent the predictions made by the network.\n",
        "The output layer applies a final activation function (like softmax for classification or linear for regression) to produce the network's output.\n",
        "Result: Forward propagation ultimately produces an output that represents the network's prediction for a given input. The loss (or error) is then calculated based on the difference between the predicted output and the actual target value.\n",
        "\n",
        "Each step of forward propagation involves matrix multiplications, additions, and the application of activation functions, which collectively enable the neural network to model complex patterns in the data.\n"
      ],
      "metadata": {
        "id": "nMIspGtXKsx0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is the purpose of the activation function in forward propagation.\n",
        "\n",
        "Ans:\n",
        "\n",
        "The purpose of the activation function in forward propagation is to introduce non-linearity into the neural network. This non-linearity is essential for enabling the network to model complex relationships and patterns within data. Without activation functions, the entire neural network would act like a single linear transformation, no matter how many layers it had, limiting its ability to solve non-linear problems.\n",
        "\n",
        "Here's how activation functions contribute to forward propagation:\n",
        "\n",
        "Non-Linearity: They allow the network to capture non-linear patterns, making it possible for the model to approximate complex functions and solve tasks like image recognition, language processing, and more.\n",
        "\n",
        "Introducing Variability: By using different types of activation functions (e.g., ReLU, sigmoid, tanh), you can control how each neuron responds to the input data, which can influence the network's learning capacity and performance.\n",
        "\n",
        "Activation Scaling: Activation functions can help scale the output, which is useful in tasks where output values need to be in a specific range, such as between 0 and 1 for probabilities (sigmoid) or between -1 and 1 (tanh).\n",
        "\n",
        "Preventing Linear Dependencies: Activation functions help avoid a situation where multiple layers could collapse into a single layer by preserving a layer-wise transformation, enabling each layer to learn different aspects of the data.\n",
        "\n",
        "Some common activation functions used in forward propagation include:\n",
        "\n",
        "ReLU (Rectified Linear Unit): Outputs the input if positive; otherwise, outputs zero. It is computationally efficient and helps mitigate the vanishing gradient problem.\n",
        "Sigmoid: Maps values to a range between 0 and 1, often used in binary classification for probabilities.\n",
        "Tanh: Maps values to a range between -1 and 1, often used when a stronger gradient is needed.\n",
        "In summary, activation functions give the network the flexibility to learn and model more complex data patterns, making them a crucial component of forward propagation."
      ],
      "metadata": {
        "id": "szoPhd1iLmBJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Describe the steps involved in the backward propagation (backpropagation) algorithm.\n",
        "\n",
        "Ans:\n",
        "\n",
        "The backpropagation algorithm is essential in training neural networks as it allows the network to minimize errors by adjusting the weights of its connections. It is the process of calculating the gradient of the loss function with respect to each weight in the network, and then using this gradient to update the weights and reduce the error. Here’s a breakdown of the key steps in backpropagation:\n",
        "\n",
        "Forward Propagation:\n",
        "\n",
        "Before backpropagation can begin, forward propagation must be performed on an input to generate predictions and compute the loss.\n",
        "The loss is calculated based on the difference between the network’s predictions and the actual target values using a loss function (e.g., mean squared error, cross-entropy).\n",
        "Calculate the Loss Gradient:\n",
        "\n",
        "The goal of backpropagation is to minimize this loss.\n",
        "Starting from the output layer, the algorithm computes the gradient of the loss function with respect to each weight by using the chain rule. The gradient indicates how much a small change in each weight will affect the loss.\n",
        "Backpropagate the Error:\n",
        "\n",
        "The error is propagated backward from the output layer through each hidden layer to the input layer. This is done to compute the gradients for all weights in the network, layer by layer.\n",
        "For each neuron in each layer, the algorithm calculates the partial derivative of the loss with respect to that neuron’s weights.\n",
        "To do this, it computes the “error” term for each neuron, which represents how much that neuron contributed to the overall error at the output.\n",
        "Calculate the Gradient for Each Weight:\n",
        "\n",
        "Using the error terms, the algorithm calculates the gradients with respect to each weight in each layer.\n",
        "This involves computing the partial derivatives of the activation functions and summing these products according to the chain rule of calculus.\n",
        "The gradients specify the direction and magnitude by which each weight should be adjusted to reduce the loss.\n",
        "Update the Weights:\n",
        "\n",
        "After calculating the gradients for all weights, the algorithm updates the weights in the direction that minimizes the loss.\n",
        "This update is typically done using a technique called gradient descent (or a variation like stochastic gradient descent or Adam optimizer).\n",
        "The learning rate, a hyperparameter, controls the size of the weight updates.\n",
        "Repeat the Process:\n",
        "\n",
        "The network continues forward and backward propagation iteratively across the training data in multiple passes (epochs).\n",
        "Each epoch reduces the loss further, allowing the network to gradually learn from the data until the loss converges or reaches an acceptable level.\n",
        "In summary, backpropagation is a combination of forward propagation to compute the loss and a backward pass to adjust weights based on the calculated gradients. This iterative process enables the neural network to \"learn\" by reducing errors in its predictions, thereby improving performance on unseen data."
      ],
      "metadata": {
        "id": "YM_T9wq7L0ZD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is the purpose of the chain rule in backpropagation.\n",
        "\n",
        "Ans:\n",
        "\n",
        "The chain rule is fundamental to backpropagation because it enables the algorithm to compute the gradient of the loss function with respect to each weight in a multi-layer neural network. This gradient, in turn, is essential for adjusting weights to minimize the loss and train the network. Here’s how the chain rule serves this purpose:\n",
        "\n",
        "Handling Multiple Layers:\n",
        "\n",
        "In a neural network, the loss depends indirectly on each weight through multiple layers of transformations. The chain rule allows backpropagation to break down this complex dependency into a series of simpler, layer-by-layer calculations.\n",
        "For each layer, the chain rule links the gradient of the loss with respect to an output to the gradient with respect to the previous layer’s weights, step-by-step from the output layer back to the input layer.\n",
        "Efficient Gradient Computation:\n",
        "\n",
        "The chain rule makes it possible to calculate the gradient for each weight in terms of the output of the layer it belongs to and the error from the subsequent layer.\n",
        "This sequential approach avoids redundant computations, making backpropagation computationally efficient, even for deep networks with many layers.\n",
        "Gradient Calculation:\n",
        "\n",
        "The chain rule essentially states that the gradient of a composition of functions is the product of the gradients of each function.\n",
        "In backpropagation, each neuron’s output and activation functions are composed functions, so the chain rule is applied to calculate the overall gradient with respect to each weight in a way that reflects how each weight indirectly affects the loss.\n",
        "Propagation of Error Signal:\n",
        "\n",
        "Backpropagation involves propagating the “error signal” (how much each neuron contributes to the error) backward through the network. The chain rule enables this by helping to express the error for each layer in terms of the error of the next layer.\n",
        "By multiplying the error with the derivatives of the activation functions along the way, the chain rule helps propagate the influence of each weight on the final loss.\n",
        "In summary, the chain rule is used in backpropagation to compute gradients layer by layer, allowing for efficient weight updates. This makes it possible for a neural network to learn by adjusting weights based on how they influence the error, even through multiple layers of complex transformations."
      ],
      "metadata": {
        "id": "jVcZOz0PMD6O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''5. Implement the forward propagation process for a simple neural network with one hidden layer using\n",
        "NumPy.'''\n",
        "\n",
        "# Code\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Define the sigmoid activation function\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Initialize the network parameters\n",
        "np.random.seed(42)  # For reproducibility\n",
        "input_size = 3      # Number of input features\n",
        "hidden_size = 4     # Number of neurons in the hidden layer\n",
        "output_size = 1     # Number of neurons in the output layer\n",
        "\n",
        "# Randomly initialize weights and biases for the network\n",
        "W1 = np.random.randn(input_size, hidden_size)  # Weights for input to hidden layer\n",
        "b1 = np.random.randn(hidden_size)              # Biases for hidden layer\n",
        "W2 = np.random.randn(hidden_size, output_size) # Weights for hidden to output layer\n",
        "b2 = np.random.randn(output_size)              # Biases for output layer\n",
        "\n",
        "# Forward propagation function\n",
        "def forward_propagation(X):\n",
        "    # Step 1: Compute the hidden layer input\n",
        "    z1 = np.dot(X, W1) + b1\n",
        "    # Step 2: Apply the activation function to hidden layer output\n",
        "    a1 = sigmoid(z1)\n",
        "    # Step 3: Compute the output layer input\n",
        "    z2 = np.dot(a1, W2) + b2\n",
        "    # Step 4: Apply the activation function to output layer output\n",
        "    output = sigmoid(z2)\n",
        "\n",
        "    return output\n",
        "\n",
        "# Example input data (batch of 2 samples with 3 features each)\n",
        "X = np.array([[0.1, 0.2, 0.3],\n",
        "              [0.4, 0.5, 0.6]])\n",
        "\n",
        "# Perform forward propagation\n",
        "output = forward_propagation(X)\n",
        "print(\"Output of the network:\")\n",
        "print(output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKRf4GvoMSN-",
        "outputId": "b40caeb2-e1f7-412f-8ab4-cd4819217658"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output of the network:\n",
            "[[0.55998689]\n",
            " [0.49571601]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3pc5x4HAMe__"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}